dnl vi: set ft=m4
/* -*- C -*-
 *
 * Copyright (c) 2019 Intel Corporation. All rights reserved.
 * This software is available to you under the BSD license.
 *
 * This file is part of the Sandia OpenSHMEM software package. For license
 * information, see the LICENSE file in the top level directory of the
 * distribution.
 *
 */

/*
 * This is a generated file, do not edit directly.
 */

include(shmem_bind_c.m4)dnl

#ifndef SHR_TRANSPORT_H
#define SHR_TRANSPORT_H

#ifdef USE_XPMEM
#include "transport_xpmem.h"
#endif

#ifdef USE_CMA
#include "transport_cma.h"
#endif

#ifdef USE_MMAP
  #include "transport_mmap.h"

  /* bman: TSX testing requires mmap */
  #define USE_TSX_ATOMIC
#endif



static inline int
shmem_shr_transport_init(void)
{
    int ret = 0;

#if USE_XPMEM
    ret = shmem_transport_xpmem_init();
    if (0 != ret)
        RETURN_ERROR_MSG("XPMEM init failed (%d)\n", ret);

#elif USE_CMA
    ret = shmem_transport_cma_init();
    if (0 != ret)
        RETURN_ERROR_MSG("CMA init failed (%d)\n", ret);

#elif USE_MMAP
    ret = shmem_transport_mmap_init();
    if (0 != ret)
        RETURN_ERROR_MSG("mmap init failed (%d)\n", ret);
#endif

    return ret;
}


static inline int
shmem_shr_transport_startup(void)
{
    int ret = 0;

#if USE_XPMEM
    ret = shmem_transport_xpmem_startup();
    if (0 != ret) {
        RETURN_ERROR_MSG("XPMEM startup failed (%d)\n", ret);
    }

#elif USE_CMA
    ret = shmem_transport_cma_startup();
    if (0 != ret) {
        RETURN_ERROR_MSG("CMA startup failed (%d)\n", ret);
    }

#elif USE_MMAP
    ret = shmem_transport_mmap_startup();
    if (0 != ret) {
        RETURN_ERROR_MSG("mmap startup failed (%d)\n", ret);
    }
#endif

    return ret;
}


static inline void
shmem_shr_transport_fini(void)
{
#if USE_XPMEM
    shmem_transport_xpmem_fini();
#elif USE_CMA
    shmem_transport_cma_fini();
#elif USE_MMAP
    shmem_transport_mmap_fini();
#endif
}


/* Return in local_ptr, a local pointer that can be used to access the target
 * buffer for the PE with noderank ID. */
static inline void
shmem_shr_transport_ptr(void *target, int noderank, void **local_ptr)
{
#if USE_XPMEM
    XPMEM_GET_REMOTE_ACCESS(target, noderank, *local_ptr);
#elif USE_MMAP
    MMAP_GET_REMOTE_ACCESS(target, noderank, *local_ptr);
#else
    RAISE_ERROR_MSG("No path to peer (%d)\n", noderank);
#endif
}


static inline int
shmem_shr_transport_use_write(shmem_ctx_t ctx, void *target, const void *source,
                              size_t len, int pe)
{
#if USE_CMA
    return  -1 != shmem_internal_get_shr_rank(pe) &&
           len <= shmem_internal_params.CMA_PUT_MAX;
#else
    return -1 != shmem_internal_get_shr_rank(pe);
#endif
}


static inline int
shmem_shr_transport_use_read(shmem_ctx_t ctx, void *target, const void *source,
                             size_t len, int pe)
{
#if USE_CMA
    return  -1 != shmem_internal_get_shr_rank(pe) &&
           len <= shmem_internal_params.CMA_GET_MAX;
#else
    return -1 != shmem_internal_get_shr_rank(pe);
#endif
}


/* Each OpenSHMEM AMO has only one symmetric pointer.  Check whether shared
 * transport AMOs are in use with respect to the given symmetric target
 * pointer and datatype. For a given datatype, all atomic operations must
 * use the same transport; therefore, op is not needed in this check. */
static inline int
shmem_shr_transport_use_atomic(shmem_ctx_t ctx, void *target, size_t len,
                               int pe, shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    return -1 != shmem_internal_get_shr_rank(pe);
#else
    return 0;
#endif
}


static inline void
shmem_shr_transport_put_scalar(shmem_ctx_t ctx, void *target,
                               const void *source, size_t len, int pe)
{
#if USE_MEMCPY
    memcpy(target, source, len);
#elif USE_XPMEM
    shmem_transport_xpmem_put(target, source, len, pe,
                              shmem_internal_get_shr_rank(pe));
#elif USE_CMA
    shmem_transport_cma_put(target, source, len, pe,
                            shmem_internal_get_shr_rank(pe));

#elif USE_MMAP
    shmem_transport_mmap_put(target, source, len, pe,
                             shmem_internal_get_shr_rank(pe));
#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline void
shmem_shr_transport_put(shmem_ctx_t ctx, void *target, const void *source,
                        size_t len, int pe)
{
#if USE_MEMCPY
    memcpy(target, source, len);
#elif USE_XPMEM
    shmem_transport_xpmem_put(target, source, len, pe,
                              shmem_internal_get_shr_rank(pe));
#elif USE_CMA
    shmem_transport_cma_put(target, source, len, pe,
                            shmem_internal_get_shr_rank(pe));

#elif USE_MMAP
    shmem_transport_mmap_put(target, source, len, pe,
                             shmem_internal_get_shr_rank(pe));
#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline void
shmem_shr_transport_get(shmem_ctx_t ctx, void *target, const void *source,
                        size_t len, int pe)
{
#if USE_MEMCPY
    memcpy(target, source, len);
#elif USE_XPMEM
    shmem_transport_xpmem_get(target, source, len, pe,
                              shmem_internal_get_shr_rank(pe));
#elif USE_CMA
    shmem_transport_cma_get(target, source, len, pe,
                            shmem_internal_get_shr_rank(pe));

#elif USE_MMAP
    shmem_transport_mmap_get(target, source, len, pe,
                             shmem_internal_get_shr_rank(pe));
#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline void
shmem_shr_transport_swap(shmem_ctx_t ctx, void *target, void *source,
                         void *dest, size_t len, int pe,
                         shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    int noderank = shmem_internal_get_shr_rank(pe);
    void *remote_ptr;

    if (noderank == -1)
        RAISE_ERROR_MSG("No shared memory path to peer %d\n", pe);

    shmem_shr_transport_ptr(target, noderank, &remote_ptr);

#define SHMEM_DEF_SWAP(STYPE,TYPE,ITYPE)                                \
        case ITYPE:                                                     \
                __atomic_exchange((TYPE *)remote_ptr, (TYPE *)source,   \
                                  (TYPE *)dest, __ATOMIC_ACQ_REL);      \
            break;

    switch (datatype) {
SHMEM_DEFINE_FOR_EXTENDED_AMO(SHMEM_DEF_SWAP)
        default:
            RAISE_ERROR_MSG("Unsupported datatype dtype=%d\n", datatype);
    }
#undef SHMEM_DEF_SWAP

#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline
void
shmem_shr_transport_cswap(shmem_ctx_t ctx, void *target, void *source,
                          void *dest, void *operand, size_t len,
                          int pe, shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    int noderank = shmem_internal_get_shr_rank(pe);
    void *remote_ptr;

    if (noderank == -1)
        RAISE_ERROR_MSG("No shared memory path to peer %d\n", pe);

    shmem_shr_transport_ptr(target, noderank, &remote_ptr);

#define SHMEM_DEF_CSWAP(STYPE,TYPE,ITYPE)                                       \
        case ITYPE:                                                             \
            {                                                                   \
                TYPE expected = *(TYPE *)operand;                               \
                bool swapped = __atomic_compare_exchange((TYPE *)remote_ptr,    \
                                                         &expected,             \
                                                         (TYPE *)source,        \
                                                         false,                 \
                                                         __ATOMIC_ACQ_REL,      \
                                                         __ATOMIC_ACQUIRE);     \
                if (swapped)                                                    \
                    *(TYPE *)dest = *(TYPE *)operand;                           \
                else                                                            \
                    /* expected contains the value of remote_ptr */             \
                    *(TYPE *)dest = expected;                                   \
            }                                                                   \
            break;

    switch (datatype) {
SHMEM_DEFINE_FOR_AMO(SHMEM_DEF_CSWAP)
        default:
            RAISE_ERROR_MSG("Unsupported datatype dtype=%d\n", datatype);
    }
#undef SHMEM_DEF_CSWAP

#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline
void
shmem_shr_transport_mswap(shmem_ctx_t ctx, void *target, void *source,
                          void *dest, void *mask, size_t len,
                          int pe, shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    int noderank = shmem_internal_get_shr_rank(pe);
    void *remote_ptr;
    bool done = false;

    if (noderank == -1)
        RAISE_ERROR_MSG("No shared memory path to peer %d\n", pe);

    shmem_shr_transport_ptr(target, noderank, &remote_ptr);

    switch (datatype) {
        case SHM_INTERNAL_INT:
            while (!done) {
                int v = __atomic_load_n((int *)remote_ptr, __ATOMIC_ACQUIRE);

                int new = ((unsigned int) v & ~*(unsigned int *)mask) |
                          (*(unsigned int *)source & *(unsigned int *)mask);

                done = __atomic_compare_exchange((int *)remote_ptr,
                                                 &v,
                                                 &new,
                                                 false,
                                                 __ATOMIC_ACQ_REL,
                                                 __ATOMIC_ACQUIRE);
                *(int *)dest = v;
            }
            break;
        default:
            RAISE_ERROR_MSG("Unsupported datatype dtype=%d\n", datatype);
    }
#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline
void
shmem_shr_transport_atomic(shmem_ctx_t ctx, void *target, const void *source,
                           size_t len, int pe, shm_internal_op_t op,
                           shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    int noderank = shmem_internal_get_shr_rank(pe);
    void *remote_ptr;

    if (noderank == -1)
        RAISE_ERROR_MSG("No shared memory path to peer %d\n", pe);

    shmem_shr_transport_ptr(target, noderank, &remote_ptr);

#define SHMEM_DEF_BAND_OP(STYPE,TYPE,ITYPE)                                                     \
                case ITYPE:                                                                     \
                    __atomic_fetch_and((TYPE *)remote_ptr, *((TYPE *)source), __ATOMIC_RELEASE);\
                    break;

#define SHMEM_DEF_BOR_OP(STYPE,TYPE,ITYPE)                                                      \
                case ITYPE:                                                                     \
                    __atomic_fetch_or((TYPE *)remote_ptr, *((TYPE *)source), __ATOMIC_RELEASE); \
                    break;

#define SHMEM_DEF_BXOR_OP(STYPE,TYPE,ITYPE)                                                     \
                case ITYPE:                                                                     \
                    __atomic_fetch_xor((TYPE *)remote_ptr, *((TYPE *)source), __ATOMIC_RELEASE);\
                    break;

#define SHMEM_DEF_SUM_OP(STYPE,TYPE,ITYPE)                                                      \
                case ITYPE:                                                                     \
                    __atomic_fetch_add((TYPE *)remote_ptr, *((TYPE *)source), __ATOMIC_RELEASE);\
                    break;

    switch (op) {
        case SHM_INTERNAL_BAND:
            switch(datatype) {
SHMEM_DEFINE_FOR_BITWISE_AMO(SHMEM_DEF_BAND_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        case SHM_INTERNAL_BOR:
            switch(datatype) {
SHMEM_DEFINE_FOR_BITWISE_AMO(SHMEM_DEF_BOR_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        case SHM_INTERNAL_BXOR:
            switch(datatype) {
SHMEM_DEFINE_FOR_BITWISE_AMO(SHMEM_DEF_BXOR_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        case SHM_INTERNAL_SUM:
            switch(datatype) {
SHMEM_DEFINE_FOR_AMO(SHMEM_DEF_SUM_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        /* Note: The following ops are only used by AMO reductions, which are
         * presently disabled when shared memory AMOs are enabled. */
        case SHM_INTERNAL_PROD:
        case SHM_INTERNAL_MIN:
        case SHM_INTERNAL_MAX:
        default:
            RAISE_ERROR_MSG("Unsupported op op=%d, dtype=%d\n", op, datatype);
    }

#undef SHMEM_DEF_BAND_OP
#undef SHMEM_DEF_BOR_OP
#undef SHMEM_DEF_BXOR_OP
#undef SHMEM_DEF_SUM_OP

#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline
void
shmem_shr_transport_atomic_fetch(shmem_ctx_t ctx, void *target,
                                 const void *source, size_t len,
                                 int pe, shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    int noderank = shmem_internal_get_shr_rank(pe);
    void *remote_ptr;

    if (noderank == -1)
        RAISE_ERROR_MSG("No shared memory path to peer %d\n", pe);

    shmem_shr_transport_ptr((void *)source, noderank, &remote_ptr);

#define SHMEM_DEF_FETCH(STYPE,TYPE,ITYPE)                                       \
        case ITYPE:                                                             \
            __atomic_load((TYPE *)remote_ptr, (TYPE *)target, __ATOMIC_ACQUIRE);\
            break;

    switch (datatype) {
SHMEM_DEFINE_FOR_EXTENDED_AMO(SHMEM_DEF_FETCH)
        default:
            RAISE_ERROR_MSG("Unsupported datatype dtype=%d\n", datatype);
    }
#undef SHMEM_DEF_FETCH

#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline
void
shmem_shr_transport_atomic_set(shmem_ctx_t ctx, void *target,
                               const void *source, size_t len,
                               int pe, shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    int noderank = shmem_internal_get_shr_rank(pe);
    void *remote_ptr;

    if (noderank == -1)
        RAISE_ERROR_MSG("No shared memory path to peer %d\n", pe);

    shmem_shr_transport_ptr(target, noderank, &remote_ptr);

#define SHMEM_DEF_SET(STYPE,TYPE,ITYPE)                                                 \
        case ITYPE:                                                                     \
            __atomic_store((TYPE *)remote_ptr, (TYPE *)source, __ATOMIC_RELEASE);       \
            break;

    switch (datatype) {
SHMEM_DEFINE_FOR_EXTENDED_AMO(SHMEM_DEF_SET)
        default:
            RAISE_ERROR_MSG("Unsupported datatype dtype=%d\n", datatype);
    }
#undef SHMEM_DEF_SET

#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline
void
shmem_shr_transport_atomicv(shmem_ctx_t ctx, void *target, const void *source,
                            size_t len, int pe, shm_internal_op_t op,
                            shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    RAISE_ERROR_STR("No path to peer");
#else
    RAISE_ERROR_STR("No path to peer");
#endif
}




// Atomic Increment using TSX

#if defined(USE_TSX_ATOMIC)

// NOTE: This needs to ultimately use SHM itself in order to work. 
//       The caller (HAMR) will call this with various pointers, so we cant simply allocate a counter internally.
//       The app would need to do this - we just operate on it. So in reality, we are tightly-coupled to the mmap solution.
//       It therefore makes more sense to just operate on the addresses, and *ASSUME* they are SHM.
//
#define BMAN_ATOMIC_INCREMENT_PROTOTYPE(TYPE_NAME, C_TYPE)                   \
  static C_TYPE bman_##TYPE_NAME##_tsx_atomic_increment(C_TYPE * remote_ptr, C_TYPE * source) { \
    int status;                                                 \
    C_TYPE retval;                                              \
    while (1) {                                                 \
        status = _xbegin();                                     \
        if (status == _XBEGIN_STARTED)                          \
        {                                                       \
            printf("Oooooo\n");                                 \
            retval = *remote_ptr;                               \
            *remote_ptr += *source;                             \
            _xend();                                            \
            break;                                              \
        }                                                       \
        else                                                    \
        {                                                       \
            retval = __atomic_fetch_add((C_TYPE *)remote_ptr, *((C_TYPE *)source), __ATOMIC_ACQ_REL); \
            break;                                              \
        }                                                       \
    }                                                           \
    return retval;                                              \
}

BMAN_ATOMIC_INCREMENT_PROTOTYPE(int, int)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(long, long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(longlong, long long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(uint, unsigned int)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(ulong, unsigned long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(ulonglong, unsigned long long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(int32, int32_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(int64, int64_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(uint32, uint32_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(uint64, uint64_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(size, size_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(ptrdiff, ptrdiff_t)

#else

#define BMAN_ATOMIC_INCREMENT_PROTOTYPE(TYPE_NAME, C_TYPE)                                      \
  static C_TYPE bman_##TYPE_NAME##_tsx_atomic_increment(C_TYPE * remote_ptr, C_TYPE * source) { \
    C_TYPE retval;                                                                              \
    retval = __atomic_fetch_add((C_TYPE *)remote_ptr, *((C_TYPE *)source), __ATOMIC_ACQ_REL);   \
    return retval;                                                                              \
}

BMAN_ATOMIC_INCREMENT_PROTOTYPE(int, int)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(long, long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(longlong, long long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(uint, unsigned int)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(ulong, unsigned long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(ulonglong, unsigned long long)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(int32, int32_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(int64, int64_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(uint32, uint32_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(uint64, uint64_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(size, size_t)
BMAN_ATOMIC_INCREMENT_PROTOTYPE(ptrdiff, ptrdiff_t)

#endif  // USE_TSX_ATOMIC


// bman: add perfmon hooks
#if defined(USE_PERFMON_ATM)
 #include <ctype.h>
 #include <stdio.h>
 #include <stdlib.h>
 #include <unistd.h>
 #include <string.h>
 #include <sys/ioctl.h>
 #include <fcntl.h>
 #include <fcntl.h>
 #include <sys/syscall.h>

static inline __attribute__((always_inline)) void start_perf_counters()
{
    if ((non_working_events & (1 << 0)) == 0)
    {
        ioctl(perf_event_fd[0], PERF_EVENT_IOC_RESET, 0);       // set counter to 0
        ioctl(perf_event_fd[0], PERF_EVENT_IOC_ENABLE, 0);      // start counter
        read(perf_event_fd[0], &start_counters[0], sizeof(start_counters[0]));
    }
    else
    {
        fprintf(stderr, "ERROR: no working events (only supprting 1)\n");
        exit(EXIT_FAILURE);
    }
}

static inline __attribute__((always_inline)) void my_start_counters() {
    start_perf_counters();
}

static inline __attribute__((always_inline)) void my_stop_counters() 
{
    if ((non_working_events & (1 << 0)) == 0) {
        ioctl(perf_event_fd[0], PERF_EVENT_IOC_DISABLE, 0);
    }
    else {
        fprintf(stderr, "ERROR: no working events.\n");
        exit(EXIT_FAILURE);
    }

    // Read result
    if ((non_working_events & (1 << 0)) == 0) {
        read(perf_event_fd[0], &end_counters[0], sizeof(end_counters[0]));
    }
    else {
        fprintf(stderr, "ERROR: no working events.\n");
        exit(EXIT_FAILURE);
    }

    // Now fill in the tracking structure
    for (int x=0; x < eventCount; x++) {
        if (non_working_events & (1 << x)) {
            eventReadBuffer[x].tdiff[diff_count] = 666;         // mark of the devil means we have an issue
        }
        else {
            eventReadBuffer[x].tdiff[diff_count] = end_counters[x] - start_counters[x];
        }
    }
    diff_count = (diff_count + 1) % MAX_COUNTS;     // rollover (moved this from inside the loop)
}

#else
 static inline __attribute__((always_inline)) void my_start_counters() {}
 static inline __attribute__((always_inline)) void my_stop_counters() {}
#endif


static inline void
shmem_shr_transport_fetch_atomic(shmem_ctx_t ctx, void *target, void *source,
                                 void *dest, size_t len, int pe,
                                 shm_internal_op_t op,
                                 shm_internal_datatype_t datatype)
{
#if USE_SHR_ATOMICS
    int noderank = shmem_internal_get_shr_rank(pe);
    void *remote_ptr;

    if (noderank == -1)
        RAISE_ERROR_MSG("No shared memory path to peer %d\n", pe);

    shmem_shr_transport_ptr(target, noderank, &remote_ptr);

#define SHMEM_DEF_BAND_OP(STYPE,TYPE,ITYPE)                                                     \
                case ITYPE:                                                                     \
                    *(TYPE *)dest = __atomic_fetch_and((TYPE *)remote_ptr, *((TYPE *)source),   \
                                                       __ATOMIC_ACQ_REL);                       \
                    break;

#define SHMEM_DEF_BOR_OP(STYPE,TYPE,ITYPE)                                                      \
                case ITYPE:                                                                     \
                    *(TYPE *)dest = __atomic_fetch_or((TYPE *)remote_ptr, *((TYPE *)source),    \
                                                      __ATOMIC_ACQ_REL);                        \
                    break;

#define SHMEM_DEF_BXOR_OP(STYPE,TYPE,ITYPE)                                                     \
                case ITYPE:                                                                     \
                    *(TYPE *)dest = __atomic_fetch_xor((TYPE *)remote_ptr, *((TYPE *)source),   \
                                                       __ATOMIC_ACQ_REL);                       \
                    break;


/* bman: in here for hamr::shmem_long_atomic_fetch_add() call */
#define SHMEM_DEF_SUM_OP(STYPE,TYPE,ITYPE)                                                      \
                case ITYPE:                                                                     \
                    /* my_start_counters(); */                                                  \
                    *(TYPE *)dest = bman_##STYPE##_tsx_atomic_increment((TYPE *)remote_ptr, (TYPE *)source); \
                                                                                                \
                    /* OG:  *(TYPE *)dest = __atomic_fetch_add((TYPE *)remote_ptr, *((TYPE *)source), */  \
                    /*                                   __ATOMIC_ACQ_REL);                      */ \
                                                                                                \
                    /* my_stop_counters(); */                                                   \
                    break;

    switch (op) {
        case SHM_INTERNAL_BAND:
            switch(datatype) {
SHMEM_DEFINE_FOR_BITWISE_AMO(SHMEM_DEF_BAND_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        case SHM_INTERNAL_BOR:
            switch(datatype) {
SHMEM_DEFINE_FOR_BITWISE_AMO(SHMEM_DEF_BOR_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        case SHM_INTERNAL_BXOR:
            switch(datatype) {
SHMEM_DEFINE_FOR_BITWISE_AMO(SHMEM_DEF_BXOR_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        case SHM_INTERNAL_SUM:
            switch(datatype) {
SHMEM_DEFINE_FOR_AMO(SHMEM_DEF_SUM_OP)
                default:
                    RAISE_ERROR_MSG("Unsupported datatype op=%d, dtype=%d\n", op, datatype);
            }
            break;
        /* Note: The following ops are only used by AMO reductions, which are
         * presently disabled when shared memory AMOs are enabled. */
        case SHM_INTERNAL_PROD:
        case SHM_INTERNAL_MIN:
        case SHM_INTERNAL_MAX:
        default:
            RAISE_ERROR_MSG("Unsupported op op=%d, dtype=%d\n", op, datatype);
    }

#undef SHMEM_DEF_BAND_OP
#undef SHMEM_DEF_BOR_OP
#undef SHMEM_DEF_BXOR_OP
#undef SHMEM_DEF_SUM_OP

#else
    RAISE_ERROR_STR("No path to peer");
#endif
}


static inline void
shmem_shr_transport_put_signal(shmem_ctx_t ctx, void *target,
                               const void *source, size_t len,
                               uint64_t *sig_addr, uint64_t signal, int sig_op, int pe)
{
#if USE_MEMCPY
    memcpy(target, source, len);
    if (sig_op == SHMEM_SIGNAL_ADD) *sig_addr += signal;
    else *sig_addr = signal;
#elif USE_XPMEM
    shmem_transport_xpmem_put(target, source, len, pe,
                              shmem_internal_get_shr_rank(pe));
    shmem_internal_membar_acq_rel(); /* Memory fence to ensure target PE observes
                                        stores in the correct order */
#elif USE_MMAP
    shmem_transport_mmap_put(target, source, len, pe,
                              shmem_internal_get_shr_rank(pe));
    shmem_internal_membar_acq_rel(); /* Memory fence to ensure target PE observes
                                        stores in the correct order */

#if USE_SHR_ATOMICS 
    if (sig_op == SHMEM_SIGNAL_ADD)
        shmem_shr_transport_atomic(ctx, sig_addr, &signal, sizeof(uint64_t),
                                   pe, SHM_INTERNAL_SUM, SHM_INTERNAL_UINT64);
    else
        shmem_shr_transport_atomic_set(ctx, sig_addr, &signal, sizeof(uint64_t),
                                       pe, SHM_INTERNAL_UINT64);
#else
    if (sig_op == SHMEM_SIGNAL_ADD)
        shmem_transport_atomic((shmem_transport_ctx_t *) ctx, sig_addr, &signal, sizeof(uint64_t),
                               pe, SHM_INTERNAL_SUM, SHM_INTERNAL_UINT64);
    else
        shmem_transport_atomic_set((shmem_transport_ctx_t *) ctx, sig_addr, &signal,
                                   sizeof(uint64_t), pe, SHM_INTERNAL_UINT64);
#endif
#elif USE_CMA
    shmem_transport_cma_put(target, source, len, pe,
                            shmem_internal_get_shr_rank(pe));
    shmem_internal_membar_acq_rel(); /* Memory fence to ensure target PE observes
                                        stores in the correct order */
    /* Using network atomics as CMA does not support atomic operations */
    if (sig_op == SHMEM_SIGNAL_ADD)
        shmem_transport_atomic((shmem_transport_ctx_t *) ctx, sig_addr, &signal, sizeof(uint64_t),
                               pe, SHM_INTERNAL_SUM, SHM_INTERNAL_UINT64);
    else
        shmem_transport_atomic_set((shmem_transport_ctx_t *) ctx, sig_addr, &signal,
                                   sizeof(uint64_t), pe, SHM_INTERNAL_UINT64);
#else
    RAISE_ERROR_STR("No path to peer");
#endif
}

#endif /* SHR_TRANSPORT_H */
